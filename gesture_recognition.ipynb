{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 01:59:33.965117: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-03 01:59:34.142024: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 1\n",
    "- Model - CNN + RNN\n",
    "- image (height, width) - (120, 120)\n",
    "- No of frame - 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = cv2.resize(image,(120, 120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = cv2.resize(image,(120, 120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 02:01:23.367123: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-03 02:01:23.553106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14928 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:1e:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (2, 2), padding='same'),\n",
    "                 input_shape=(x, y, z, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(16, (2, 2))))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (2, 2), padding='same')))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(32, (2, 2), padding='same')))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(256, return_sequences=False, dropout=0.5))\n",
    "model.add(Dense(64,kernel_regularizer=l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 30, 120, 120, 16)  208      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 30, 120, 120, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 30, 120, 120, 16)  64       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 30, 119, 119, 16)  1040     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 30, 119, 119, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 119, 119, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 30, 59, 59, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 30, 59, 59, 32)   2080      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 30, 59, 59, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 30, 59, 59, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 30, 59, 59, 32)   4128      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 30, 59, 59, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 30, 59, 59, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 30, 29, 29, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 29, 29, 32)    0         \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 30, 26912)        0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               27821056  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,845,669\n",
      "Trainable params: 27,845,477\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# optimiser = 'adam'#write your optimizer\n",
    "# model.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# print (model.summary())\n",
    "\n",
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 02:02:10.885781: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 14s - loss: 2.4234 - categorical_accuracy: 0.3281Batch:  34 Index: 20\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.4112 - categorical_accuracy: 0.3303Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 280s 8s/step - loss: 2.4112 - categorical_accuracy: 0.3303 - val_loss: 2.8546 - val_categorical_accuracy: 0.2100\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 16s 462ms/step - loss: 2.0265 - categorical_accuracy: 0.3922 - val_loss: 2.3254 - val_categorical_accuracy: 0.1500\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 16s 476ms/step - loss: 1.8765 - categorical_accuracy: 0.3922 - val_loss: 2.1679 - val_categorical_accuracy: 0.1600\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 16s 480ms/step - loss: 1.7461 - categorical_accuracy: 0.4412 - val_loss: 2.0998 - val_categorical_accuracy: 0.1700\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 15s 454ms/step - loss: 1.6751 - categorical_accuracy: 0.4118 - val_loss: 2.3992 - val_categorical_accuracy: 0.2000\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 15s 452ms/step - loss: 1.4822 - categorical_accuracy: 0.4902 - val_loss: 2.2834 - val_categorical_accuracy: 0.2600\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 16s 480ms/step - loss: 1.5011 - categorical_accuracy: 0.5098 - val_loss: 2.1543 - val_categorical_accuracy: 0.3100\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 15s 438ms/step - loss: 1.4135 - categorical_accuracy: 0.5980 - val_loss: 2.3062 - val_categorical_accuracy: 0.2000\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 14s 433ms/step - loss: 1.3568 - categorical_accuracy: 0.5294 - val_loss: 2.2978 - val_categorical_accuracy: 0.1900\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 15s 463ms/step - loss: 1.4146 - categorical_accuracy: 0.5588 - val_loss: 1.9601 - val_categorical_accuracy: 0.2800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5f28e9700>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "#                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "\n",
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, validation_data=val_generator, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 2\n",
    "- Model - CNN + RNN\n",
    "- image (height, width) - (120, 120)\n",
    "- No of frame - 15\n",
    "- Batch size - 40\n",
    "- No of epochs - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 15 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    #img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    img_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = cv2.resize(image,(120, 120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = cv2.resize(image,(120, 120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (2, 2), padding='same'),\n",
    "                 input_shape=(15, 120, 120, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(16, (2, 2))))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (2, 2), padding='same')))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(32, (2, 2), padding='same')))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(256, return_sequences=False, dropout=0.5))\n",
    "model.add(Dense(64,kernel_regularizer=l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_7 (TimeDis  (None, 15, 120, 120, 16)  208      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 15, 120, 120, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 15, 120, 120, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 15, 119, 119, 16)  1040     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 15, 119, 119, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 15, 119, 119, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 15, 59, 59, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 15, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 15, 59, 59, 32)   2080      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 15, 59, 59, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 15, 59, 59, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 15, 59, 59, 32)   4128      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 15, 59, 59, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 15, 59, 59, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 15, 29, 29, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 15, 29, 29, 32)    0         \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 15, 26912)        0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               27821056  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,845,669\n",
      "Trainable params: 27,845,477\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 40\n",
      "Epoch 1/20\n",
      "15/17 [=========================>....] - ETA: 2s - loss: 2.4428 - categorical_accuracy: 0.3933Batch:  17 Index: 40\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.4108 - categorical_accuracy: 0.4118Source path =  Project_data/val ; batch size = 40\n",
      "Batch:  3 Index: 40\n",
      "17/17 [==============================] - 28s 2s/step - loss: 2.4108 - categorical_accuracy: 0.4118 - val_loss: 2.5387 - val_categorical_accuracy: 0.2300\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 15s 886ms/step - loss: 1.9199 - categorical_accuracy: 0.5857 - val_loss: 2.4702 - val_categorical_accuracy: 0.2500\n",
      "Epoch 3/20\n",
      "10/17 [================>.............] - ETA: 5s - loss: 1.7164 - categorical_accuracy: 0.6217Batch:  29 Index: 23\n",
      "17/17 [==============================] - 14s 878ms/step - loss: 1.6101 - categorical_accuracy: 0.6676 - val_loss: 2.4376 - val_categorical_accuracy: 0.2833\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 13s 789ms/step - loss: 1.3855 - categorical_accuracy: 0.6780 - val_loss: 2.8564 - val_categorical_accuracy: 0.1167\n",
      "Epoch 5/20\n",
      "11/17 [==================>...........] - ETA: 3s - loss: 1.2371 - categorical_accuracy: 0.7273Batch:  35 Index: 19\n",
      "17/17 [==============================] - 13s 798ms/step - loss: 1.2021 - categorical_accuracy: 0.7284 - val_loss: 2.7442 - val_categorical_accuracy: 0.2500\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 12s 733ms/step - loss: 1.1966 - categorical_accuracy: 0.7336 - val_loss: 2.5926 - val_categorical_accuracy: 0.1833\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 12s 728ms/step - loss: 1.1023 - categorical_accuracy: 0.6990 - val_loss: 2.4971 - val_categorical_accuracy: 0.2500\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 12s 726ms/step - loss: 0.9637 - categorical_accuracy: 0.8339 - val_loss: 2.5345 - val_categorical_accuracy: 0.2833\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 12s 760ms/step - loss: 0.8831 - categorical_accuracy: 0.8028 - val_loss: 2.6515 - val_categorical_accuracy: 0.2000\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 12s 717ms/step - loss: 0.7726 - categorical_accuracy: 0.8547 - val_loss: 2.5399 - val_categorical_accuracy: 0.2500\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 12s 731ms/step - loss: 0.8042 - categorical_accuracy: 0.8201 - val_loss: 2.9849 - val_categorical_accuracy: 0.2167\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 12s 722ms/step - loss: 0.7613 - categorical_accuracy: 0.8443 - val_loss: 3.0654 - val_categorical_accuracy: 0.2833\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 12s 751ms/step - loss: 0.6333 - categorical_accuracy: 0.8858 - val_loss: 2.8130 - val_categorical_accuracy: 0.2833\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 12s 719ms/step - loss: 0.6090 - categorical_accuracy: 0.9066 - val_loss: 2.9245 - val_categorical_accuracy: 0.2333\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 12s 752ms/step - loss: 0.5604 - categorical_accuracy: 0.9239 - val_loss: 3.6314 - val_categorical_accuracy: 0.2167\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 12s 759ms/step - loss: 0.5330 - categorical_accuracy: 0.8997 - val_loss: 3.4361 - val_categorical_accuracy: 0.2833\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 12s 732ms/step - loss: 0.4973 - categorical_accuracy: 0.9273 - val_loss: 3.5957 - val_categorical_accuracy: 0.2333\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 12s 731ms/step - loss: 0.4844 - categorical_accuracy: 0.9377 - val_loss: 3.1588 - val_categorical_accuracy: 0.3000\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 12s 726ms/step - loss: 0.4326 - categorical_accuracy: 0.9481 - val_loss: 2.8121 - val_categorical_accuracy: 0.3000\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 12s 748ms/step - loss: 0.4090 - categorical_accuracy: 0.9550 - val_loss: 2.9041 - val_categorical_accuracy: 0.3167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4a1cb5fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, validation_data=val_generator, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 2\n",
    "- Model - Convolutional3D\n",
    "- image (height, width) - (120, 120)\n",
    "- No of frame - 30\n",
    "- Batch size - 20\n",
    "- No of epochs - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    #img_idx = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = cv2.resize(image,(120, 120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = cv2.resize(image,(120, 120))\n",
    "                    temp = temp/127.5-1 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, #number of filters \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=(x, y, z , 3),\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, #Number of filters, \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, #Number of filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, #Number pf filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 30, 120, 120, 8)  32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 15, 60, 60, 8)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 15, 60, 60, 16)   64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 7, 30, 30, 16)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 7, 30, 30, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 3, 15, 15, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 3, 15, 15, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 1, 7, 7, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 20\n",
      "Epoch 1/10\n",
      "32/34 [===========================>..] - ETA: 2s - loss: 4.5997 - categorical_accuracy: 0.2750Batch:  34 Index: 20\n",
      "34/34 [==============================] - ETA: 0s - loss: 4.5259 - categorical_accuracy: 0.2745Source path =  Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 51s 1s/step - loss: 4.5259 - categorical_accuracy: 0.2745 - val_loss: 1.8280 - val_categorical_accuracy: 0.2400\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 15s 453ms/step - loss: 2.4177 - categorical_accuracy: 0.2843 - val_loss: 1.9744 - val_categorical_accuracy: 0.1900\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 16s 475ms/step - loss: 2.5438 - categorical_accuracy: 0.3039 - val_loss: 1.4880 - val_categorical_accuracy: 0.3300\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 14s 429ms/step - loss: 1.7958 - categorical_accuracy: 0.3333 - val_loss: 1.5592 - val_categorical_accuracy: 0.2400\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 15s 464ms/step - loss: 1.6389 - categorical_accuracy: 0.3529 - val_loss: 1.3217 - val_categorical_accuracy: 0.4500\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 15s 460ms/step - loss: 1.7370 - categorical_accuracy: 0.2745 - val_loss: 1.5397 - val_categorical_accuracy: 0.3000\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 15s 458ms/step - loss: 1.6873 - categorical_accuracy: 0.2843 - val_loss: 1.2875 - val_categorical_accuracy: 0.4300\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 15s 467ms/step - loss: 1.7075 - categorical_accuracy: 0.2353 - val_loss: 1.2824 - val_categorical_accuracy: 0.4000\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 15s 452ms/step - loss: 1.5878 - categorical_accuracy: 0.2745 - val_loss: 1.3882 - val_categorical_accuracy: 0.5400\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 15s 459ms/step - loss: 1.4218 - categorical_accuracy: 0.3725 - val_loss: 1.2144 - val_categorical_accuracy: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4a2774b20>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, validation_data=val_generator, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 4\n",
    "- Model - Convolutional3D\n",
    "- image (height, width) - (120, 120)\n",
    "- No of frame - 30\n",
    "- Batch size - 40\n",
    "- No of epochs - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_epochs = 20\n",
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    #img_idx = np.arange(0, 30, 3)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = cv2.resize(image,(120, 120))\n",
    "                    temp = temp/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = cv2.resize(image,(120, 120))\n",
    "                    temp = temp/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, #number of filters \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=(x, y, z , 3),\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, #Number of filters, \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, #Number of filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, #Number pf filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_24 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 30, 120, 120, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d_24 (MaxPoolin  (None, 15, 60, 60, 8)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_25 (Conv3D)          (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 15, 60, 60, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_25 (MaxPoolin  (None, 7, 30, 30, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_26 (Conv3D)          (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 7, 30, 30, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_26 (MaxPoolin  (None, 3, 15, 15, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_27 (Conv3D)          (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 3, 15, 15, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_27 (MaxPoolin  (None, 1, 7, 7, 64)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 40\n",
      "Epoch 1/20\n",
      "15/17 [=========================>....] - ETA: 4s - loss: 4.8014 - categorical_accuracy: 0.2783Batch:  17 Index: 40\n",
      "17/17 [==============================] - ETA: 0s - loss: 4.6628 - categorical_accuracy: 0.2775Source path =  Project_data/val ; batch size = 40\n",
      "Batch:  3 Index: 40\n",
      "17/17 [==============================] - 48s 3s/step - loss: 4.6628 - categorical_accuracy: 0.2775 - val_loss: 1.6467 - val_categorical_accuracy: 0.2500\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.9496 - categorical_accuracy: 0.3274 - val_loss: 1.5105 - val_categorical_accuracy: 0.3167\n",
      "Epoch 3/20\n",
      "10/17 [================>.............] - ETA: 9s - loss: 1.4341 - categorical_accuracy: 0.4087 Batch:  29 Index: 23\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.3354 - categorical_accuracy: 0.4687 - val_loss: 1.5251 - val_categorical_accuracy: 0.2333\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.2994 - categorical_accuracy: 0.4706 - val_loss: 1.5599 - val_categorical_accuracy: 0.3167\n",
      "Epoch 5/20\n",
      "11/17 [==================>...........] - ETA: 6s - loss: 1.2134 - categorical_accuracy: 0.4928Batch:  35 Index: 19\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.2523 - categorical_accuracy: 0.4824 - val_loss: 1.8024 - val_categorical_accuracy: 0.2500\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 1.1218 - categorical_accuracy: 0.5536 - val_loss: 1.7955 - val_categorical_accuracy: 0.3333\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 21s 1s/step - loss: 1.0607 - categorical_accuracy: 0.5779 - val_loss: 2.0241 - val_categorical_accuracy: 0.2167\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.9854 - categorical_accuracy: 0.5917 - val_loss: 1.9218 - val_categorical_accuracy: 0.2333\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.9441 - categorical_accuracy: 0.6194 - val_loss: 2.2680 - val_categorical_accuracy: 0.2667\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.9185 - categorical_accuracy: 0.6471 - val_loss: 1.9811 - val_categorical_accuracy: 0.3667\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.7728 - categorical_accuracy: 0.6851 - val_loss: 2.4154 - val_categorical_accuracy: 0.2500\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.7655 - categorical_accuracy: 0.6747 - val_loss: 2.5442 - val_categorical_accuracy: 0.2667\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.7737 - categorical_accuracy: 0.7405 - val_loss: 2.1803 - val_categorical_accuracy: 0.3000\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 21s 1s/step - loss: 0.7507 - categorical_accuracy: 0.7266 - val_loss: 3.2136 - val_categorical_accuracy: 0.2667\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.6702 - categorical_accuracy: 0.7509 - val_loss: 2.3578 - val_categorical_accuracy: 0.2000\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.7328 - categorical_accuracy: 0.7578 - val_loss: 2.2679 - val_categorical_accuracy: 0.3000\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.5808 - categorical_accuracy: 0.7647 - val_loss: 1.8761 - val_categorical_accuracy: 0.4333\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.5500 - categorical_accuracy: 0.8028 - val_loss: 1.8505 - val_categorical_accuracy: 0.4500\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 21s 1s/step - loss: 0.5234 - categorical_accuracy: 0.8270 - val_loss: 1.6733 - val_categorical_accuracy: 0.4667\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 22s 1s/step - loss: 0.5027 - categorical_accuracy: 0.8166 - val_loss: 1.4452 - val_categorical_accuracy: 0.5167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4a0a97e80>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, validation_data=val_generator, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 5\n",
    "- Model - Convolutional3D\n",
    "- image (height, width) - (60, 60)\n",
    "- No of frame - 30\n",
    "- Batch size - 40\n",
    "- No of epochs - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "num_epochs = 20\n",
    "x = 30 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    #img_idx = np.arange(0, 30, 3)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = cv2.resize(image,(y, z))\n",
    "                    temp = temp/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = cv2.resize(image,(y, z))\n",
    "                    temp = temp/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, #number of filters \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=(x, y, z , 3),\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, #Number of filters, \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, #Number of filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, #Number pf filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_28 (Conv3D)          (None, 30, 60, 60, 8)     656       \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 30, 60, 60, 8)    32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 30, 60, 60, 8)     0         \n",
      "                                                                 \n",
      " max_pooling3d_28 (MaxPoolin  (None, 15, 30, 30, 8)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_29 (Conv3D)          (None, 15, 30, 30, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 15, 30, 30, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 15, 30, 30, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_29 (MaxPoolin  (None, 7, 15, 15, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_30 (Conv3D)          (None, 7, 15, 15, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 7, 15, 15, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 7, 15, 15, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_30 (MaxPoolin  (None, 3, 7, 7, 32)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_31 (Conv3D)          (None, 3, 7, 7, 64)       18496     \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 3, 7, 7, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_43 (Activation)  (None, 3, 7, 7, 64)       0         \n",
      "                                                                 \n",
      " max_pooling3d_31 (MaxPoolin  (None, 1, 3, 3, 64)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1000)              577000    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,107,749\n",
      "Trainable params: 1,107,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 40\n",
      "Epoch 1/20\n",
      "15/17 [=========================>....] - ETA: 4s - loss: 2.2829 - categorical_accuracy: 0.2867Batch:  17 Index: 40\n",
      "17/17 [==============================] - ETA: 0s - loss: 2.2061 - categorical_accuracy: 0.2986Source path =  Project_data/val ; batch size = 40\n",
      "Batch:  3 Index: 40\n",
      "17/17 [==============================] - 41s 2s/step - loss: 2.2061 - categorical_accuracy: 0.2986 - val_loss: 1.5720 - val_categorical_accuracy: 0.2600\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.4732 - categorical_accuracy: 0.3734 - val_loss: 1.6207 - val_categorical_accuracy: 0.2333\n",
      "Epoch 3/20\n",
      "10/17 [================>.............] - ETA: 7s - loss: 1.4490 - categorical_accuracy: 0.4087Batch:  29 Index: 23\n",
      "17/17 [==============================] - 22s 1s/step - loss: 1.3632 - categorical_accuracy: 0.4469 - val_loss: 1.5210 - val_categorical_accuracy: 0.2833\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 20s 1s/step - loss: 1.2286 - categorical_accuracy: 0.5015 - val_loss: 1.6486 - val_categorical_accuracy: 0.2667\n",
      "Epoch 5/20\n",
      "11/17 [==================>...........] - ETA: 5s - loss: 1.0568 - categorical_accuracy: 0.5407Batch:  35 Index: 19\n",
      "17/17 [==============================] - 21s 1s/step - loss: 1.0417 - categorical_accuracy: 0.5559 - val_loss: 2.1218 - val_categorical_accuracy: 0.2333\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.9681 - categorical_accuracy: 0.5952 - val_loss: 2.5159 - val_categorical_accuracy: 0.2333\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 1.0408 - categorical_accuracy: 0.5744 - val_loss: 2.3213 - val_categorical_accuracy: 0.1667\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 18s 1s/step - loss: 0.8265 - categorical_accuracy: 0.6713 - val_loss: 2.4018 - val_categorical_accuracy: 0.1833\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.9885 - categorical_accuracy: 0.5502 - val_loss: 1.9516 - val_categorical_accuracy: 0.3333\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.8078 - categorical_accuracy: 0.6747 - val_loss: 2.9684 - val_categorical_accuracy: 0.2667\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.7943 - categorical_accuracy: 0.7024 - val_loss: 4.2598 - val_categorical_accuracy: 0.2333\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 18s 1s/step - loss: 0.6664 - categorical_accuracy: 0.7266 - val_loss: 2.8915 - val_categorical_accuracy: 0.2500\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 18s 1s/step - loss: 0.6808 - categorical_accuracy: 0.7128 - val_loss: 4.4484 - val_categorical_accuracy: 0.1667\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.5081 - categorical_accuracy: 0.8166 - val_loss: 4.0978 - val_categorical_accuracy: 0.2333\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.5258 - categorical_accuracy: 0.7889 - val_loss: 2.6547 - val_categorical_accuracy: 0.2667\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.4830 - categorical_accuracy: 0.8339 - val_loss: 2.3682 - val_categorical_accuracy: 0.2667\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.4331 - categorical_accuracy: 0.8374 - val_loss: 1.0887 - val_categorical_accuracy: 0.5500\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 18s 1s/step - loss: 0.4317 - categorical_accuracy: 0.8512 - val_loss: 3.1403 - val_categorical_accuracy: 0.2500\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 19s 1s/step - loss: 0.5223 - categorical_accuracy: 0.8062 - val_loss: 1.6462 - val_categorical_accuracy: 0.4333\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 20s 1s/step - loss: 0.3754 - categorical_accuracy: 0.8581 - val_loss: 1.5659 - val_categorical_accuracy: 0.5167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4a0a3f400>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, validation_data=val_generator, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 6\n",
    "- Model - Convolutional3D\n",
    "- image (height, width) - (120, 120)\n",
    "- No of frame - 30\n",
    "- Batch size - 60\n",
    "- No of epochs - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 120 # image width\n",
    "z = 120 # image height\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,x)] #create a list of image numbers you want to use for a particular video\n",
    "    #img_idx = np.arange(0, 30, 3)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = cv2.resize(image,(y, z))\n",
    "                    temp = temp/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1]) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2]) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = cv2.resize(image,(y, z))\n",
    "                    temp = temp/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, #number of filters \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=(x, y, z , 3),\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, #Number of filters, \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, #Number of filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, #Number pf filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_32 (Conv3D)          (None, 30, 120, 120, 8)   656       \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 30, 120, 120, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_44 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " max_pooling3d_32 (MaxPoolin  (None, 15, 60, 60, 8)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_33 (Conv3D)          (None, 15, 60, 60, 16)    3472      \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 15, 60, 60, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_45 (Activation)  (None, 15, 60, 60, 16)    0         \n",
      "                                                                 \n",
      " max_pooling3d_33 (MaxPoolin  (None, 7, 30, 30, 16)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_34 (Conv3D)          (None, 7, 30, 30, 32)     4640      \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 7, 30, 30, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 7, 30, 30, 32)     0         \n",
      "                                                                 \n",
      " max_pooling3d_34 (MaxPoolin  (None, 3, 15, 15, 32)    0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " conv3d_35 (Conv3D)          (None, 3, 15, 15, 64)     18496     \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 3, 15, 15, 64)    256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 3, 15, 15, 64)     0         \n",
      "                                                                 \n",
      " max_pooling3d_35 (MaxPoolin  (None, 1, 7, 7, 64)      0         \n",
      " g3D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1000)              3137000   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 500)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 5)                 2505      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.001) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)# write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 60\n",
      "Epoch 1/20\n",
      "10/12 [========================>.....] - ETA: 6s - loss: 6.7261 - categorical_accuracy: 0.2267Batch:  12 Index: 60\n",
      "11/12 [==========================>...] - ETA: 3s - loss: 6.4218 - categorical_accuracy: 0.2333Source path =  Project_data/val ; batch size = 60\n",
      "Batch:  2 Index: 60\n",
      "12/12 [==============================] - 45s 4s/step - loss: 6.4069 - categorical_accuracy: 0.2353 - val_loss: 1.6983 - val_categorical_accuracy: 0.2100\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 3.9678 - categorical_accuracy: 0.1667Batch:  3 Index: 40\n",
      "12/12 [==============================] - 7s 657ms/step - loss: 3.9678 - categorical_accuracy: 0.1667 - val_loss: 2.0018 - val_categorical_accuracy: 0.2500\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 3.1584 - categorical_accuracy: 0.3056 - val_loss: 1.7707 - val_categorical_accuracy: 0.1500\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 7s 647ms/step - loss: 2.8662 - categorical_accuracy: 0.3611 - val_loss: 2.6387 - val_categorical_accuracy: 0.1500\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 6s 564ms/step - loss: 3.2638 - categorical_accuracy: 0.3333 - val_loss: 1.6596 - val_categorical_accuracy: 0.2750\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 6s 532ms/step - loss: 2.6799 - categorical_accuracy: 0.1944 - val_loss: 1.5217 - val_categorical_accuracy: 0.3750\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - 6s 541ms/step - loss: 2.4309 - categorical_accuracy: 0.2778 - val_loss: 1.5560 - val_categorical_accuracy: 0.3250\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - 6s 538ms/step - loss: 2.0911 - categorical_accuracy: 0.3611 - val_loss: 1.6051 - val_categorical_accuracy: 0.3250\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - 6s 564ms/step - loss: 2.1794 - categorical_accuracy: 0.1944 - val_loss: 1.7346 - val_categorical_accuracy: 0.1250\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - 6s 584ms/step - loss: 1.6690 - categorical_accuracy: 0.2222 - val_loss: 1.7222 - val_categorical_accuracy: 0.1750\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - 6s 565ms/step - loss: 1.8784 - categorical_accuracy: 0.1389 - val_loss: 1.6428 - val_categorical_accuracy: 0.2750\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - 6s 514ms/step - loss: 1.6764 - categorical_accuracy: 0.2500 - val_loss: 1.7085 - val_categorical_accuracy: 0.2500\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - 6s 584ms/step - loss: 1.5575 - categorical_accuracy: 0.3333 - val_loss: 1.8507 - val_categorical_accuracy: 0.2000\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - 6s 516ms/step - loss: 1.6174 - categorical_accuracy: 0.2778 - val_loss: 1.8175 - val_categorical_accuracy: 0.1750\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 1.4668 - categorical_accuracy: 0.3611 - val_loss: 1.8064 - val_categorical_accuracy: 0.2250\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - 6s 543ms/step - loss: 1.6223 - categorical_accuracy: 0.2778 - val_loss: 1.6989 - val_categorical_accuracy: 0.2750\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - 6s 526ms/step - loss: 1.6054 - categorical_accuracy: 0.2500 - val_loss: 1.8013 - val_categorical_accuracy: 0.2250\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - 6s 560ms/step - loss: 1.3434 - categorical_accuracy: 0.3889 - val_loss: 1.8657 - val_categorical_accuracy: 0.3250\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - 6s 566ms/step - loss: 1.4426 - categorical_accuracy: 0.4444 - val_loss: 1.9375 - val_categorical_accuracy: 0.2000\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - 6s 550ms/step - loss: 1.5478 - categorical_accuracy: 0.3889 - val_loss: 1.6185 - val_categorical_accuracy: 0.3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4a05a2ac0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, validation_data=val_generator, validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
